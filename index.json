[{"categories":["Markdown"],"content":"Learning basic Markdown syntax and formatting to make your content look great.","date":"2019-11-29","objectID":"/syntax/","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"This article offers a quick cheatsheet of basic Markdown syntax that can be used in Hugo content files. Note This article is a shameless plug of many great articles and documentations, in order to consolidate a better markdown cheatsheet for my hugo blog endeavours. Grav markdown. Basic Syntax. Extended Syntax. Over the years I have found writing content in Markdown or reStructureText is way more time efficent as compared to writing html. WYSIWYG editors will help reduce the complexity of this task but they can result in not so good looking web pages. Markdown is a better way to write HTML, without all the complexities and ugliness that usually accompanies it. Some benefits that can be attributed to Markdown are: Markdown is simple to learn, with minimal extra characters, so it’s also quicker to write content. Less chance of errors when writing in Markdown. Produces valid XHTML output. Keeps the content and the visual display separate, so you cannot mess up the look of your site. Write in any text editor or Markdown application you like. Markdown is a joy to use! John Gruber, the author of Markdown, puts it like this: The overriding design goal for Markdown’s formatting syntax is to make it as readable as possible. The idea is that a Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. While Markdown’s syntax has been influenced by several existing text-to-HTML filters, the single biggest source of inspiration for Markdown’s syntax is the format of plain text email. – John Gruber Without further delay, let us go over the main elements of Markdown and what the resulting HTML looks like! Tip  Bookmark this page for easy future reference! ","date":"2019-11-29","objectID":"/syntax/:0:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Headings Headings from h2 through h6 are constructed with a # for each level: ## h2 Heading ### h3 Heading #### h4 Heading ##### h5 Heading ###### h6 Heading The HTML looks like this: \u003ch2\u003eh2 Heading\u003c/h2\u003e \u003ch3\u003eh3 Heading\u003c/h3\u003e \u003ch4\u003eh4 Heading\u003c/h4\u003e \u003ch5\u003eh5 Heading\u003c/h5\u003e \u003ch6\u003eh6 Heading\u003c/h6\u003e Heading IDs To add a custom heading ID, enclose the custom ID in curly braces on the same line as the heading: ### A Great Heading {#custom-id} The HTML looks like this: \u003ch3 id=\"custom-id\"\u003eA Great Heading\u003c/h3\u003e ","date":"2019-11-29","objectID":"/syntax/:1:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Alternate Syntax Alternatively, on the line below the text, add any number of == characters for heading level 1 or – characters for heading level 2. h1 Heading ========== h2 Heading ---------- The HTML looks like this: \u003ch1\u003eHeading level 1\u003c/h1\u003e \u003ch2\u003eHeading level 2\u003c/h2\u003e ","date":"2019-11-29","objectID":"/syntax/:1:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Best Practices Markdown applications don’t agree on how to handle missing blank lines between a heading and the surrounding paragraphs. For compatibility, separate paragraphs and headings with one or more blank lines. ✅ Recommended ❌ Avoid This is a paragraph. # Here’s the heading And this is another paragraph. This is a paragraph. # Here’s the heading And this is another paragraph. ","date":"2019-11-29","objectID":"/syntax/:1:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Paragraphs To create paragraphs, use a blank line to separate one or more lines of text. I really like using Markdown. I think I'll use it to format all of my documents from now on. The HTML looks like this: \u003cp\u003eI really like using Markdown.\u003c/p\u003e \u003cp\u003eI think I'll use it to format all of my documents from now on.\u003c/p\u003e The rendered output looks like this: I really like using Markdown. I think I’ll use it to format all of my documents from now on. ","date":"2019-11-29","objectID":"/syntax/:2:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Best Practices Don’t indent paragraphs with spaces or tabs. ✅ Recommended ❌ Avoid Don’t put tabs or spaces in front of your paragraphs. Keep lines left-aligned like this.  This can result in unexpected formatting problems. Don’t add tabs or spaces in front of paragraphs. ","date":"2019-11-29","objectID":"/syntax/:2:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Line Breaks To create a line break (\u003cbr\u003e), end a line with two or more spaces, and then type return. This is the first line. And this is the second line. The HTML looks like this: \u003cp\u003eThis is the first line.\u003cbr\u003e And this is the second line.\u003c/p\u003e The rendered output looks like this: This is the first line. And this is the second line. ","date":"2019-11-29","objectID":"/syntax/:3:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Best Practices You can use two or more spaces (referred to as “trailing whitespace”) for line breaks in nearly every Markdown application, but it’s controversial. It’s hard to see trailing whitespace in an editor, and many people accidentally or intentionally put two spaces after every sentence. For this reason, you may want to use something other than trailing whitespace for line breaks. Fortunately, there is another option supported by nearly every Markdown application: the \u003cbr\u003e HTML tag. For compatibility, use trailing white space or the \u003cbr\u003e HTML tag at the end of the line. There are two other options I don’t recommend using. CommonMark and a few other lightweight markup languages let you type a backslash () at the end of the line, but not all Markdown applications support this, so it isn’t a great option from a compatibility perspective. And at least a couple lightweight markup languages don’t require anything at the end of the line — just type return and they’ll create a line break. ✅ Recommended ❌ Avoid First line with two spaces after. And the next line. First line with the HTML tag after.\u003cbr\u003e And the next line. First line with a backslash after.\\ And the next line. First line with nothing after. And the next line. ","date":"2019-11-29","objectID":"/syntax/:3:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Comment Comments should be HTML compatible. \u003c!-- This is a comment --\u003e Comment below should NOT be seen: ","date":"2019-11-29","objectID":"/syntax/:4:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Horizontal Rules The HTML \u003chr\u003e element is for creating a “thematic break” between paragraph-level elements. In Markdown, you can create a \u003chr\u003e with any of the following: ___: three consecutive underscores ---: three consecutive dashes ***: three consecutive asterisks The rendered output looks like this: ","date":"2019-11-29","objectID":"/syntax/:5:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Best Practices ✅ Recommended ❌ Avoid Try to put a blank line before… - - - …and after a horizontal rule. Without blank lines, this would be a heading. - - - Don’t do this! ","date":"2019-11-29","objectID":"/syntax/:5:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Body Copy Body copy written as normal, plain text will be wrapped with \u003cp\u003e\u003c/p\u003e tags in the rendered HTML. So this body copy: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. The HTML looks like this: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e A line break can be done with one blank line. ","date":"2019-11-29","objectID":"/syntax/:6:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Inline HTML If you need a certain HTML tag (with a class) you can simply use HTML: Paragraph in Markdown. \u003cdiv class=\"class\"\u003e This is \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Paragraph in Markdown. ","date":"2019-11-29","objectID":"/syntax/:7:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Emphasis ","date":"2019-11-29","objectID":"/syntax/:8:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Bold For emphasizing a snippet of text with a heavier font-weight. The following snippet of text is rendered as bold text. **rendered as bold text** __rendered as bold text__ rendered as **bold** text The HTML looks like this: \u003cstrong\u003erendered as bold text\u003c/strong\u003e rendered as \u003cstrong\u003ebold\u003c/strong\u003e text Best Practices Markdown applications don’t agree on how to handle underscores in the middle of a word. For compatibility, use asterisks to bold the middle of a word for emphasis. ✅ Recommended ❌ Avoid Love**is**bold Love__is__bold ","date":"2019-11-29","objectID":"/syntax/:8:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Italics For emphasizing a snippet of text with italics. The following snippet of text is rendered as italicized text. *rendered as italicized text* _rendered as italicized text_ rendered as *italicized* text The HTML looks like this: \u003cem\u003erendered as italicized text\u003c/em\u003e rendered as \u003cem\u003eitalicized\u003c/em\u003e text Best Practices Markdown applications don’t agree on how to handle underscores in the middle of a word. For compatibility, use asterisks to italicize the middle of a word for emphasis. ✅ Recommended ❌ Avoid Love*is*bold Love_is_bold ","date":"2019-11-29","objectID":"/syntax/:8:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Strikethrough In GFMGitHub flavored Markdown you can do strikethroughs. You can strikethrough words by putting a horizontal line through the center of them. The result looks like this. This feature allows you to indicate that certain words are a mistake not meant for inclusion in the document. To strikethrough words, use two tilde symbols (~~) before and after the words. ~~Strike through this text.~~ The rendered output looks like this: Strike through this text. The HTML looks like this: \u003cdel\u003eStrike through this text.\u003c/del\u003e ","date":"2019-11-29","objectID":"/syntax/:8:3","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Combination Bold, italics, and strikethrough can be used in combination. ***bold and italics*** ~~**strikethrough and bold**~~ ~~*strikethrough and italics*~~ ~~***bold, italics and strikethrough***~~ The rendered output looks like this: bold and italics strikethrough and bold strikethrough and italics bold, italics and strikethrough The HTML looks like this: \u003cem\u003e\u003cstrong\u003ebold and italics\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003estrikethrough and bold\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003estrikethrough and italics\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003ebold, italics and strikethrough\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-11-29","objectID":"/syntax/:8:4","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Blockquotes For quoting blocks of content from another source within your document. Add \u003e before any text you want to quote: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. The rendered output looks like this: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. The HTML looks like this: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e ","date":"2019-11-29","objectID":"/syntax/:9:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Blockquotes with Multiple Paragraphs Blockquotes can contain multiple paragraphs. Add a \u003e on the blank lines between the paragraphs. \u003e Dorothy followed her through many of the beautiful rooms in her castle. \u003e \u003e The Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood. The rendered output looks like this: Dorothy followed her through many of the beautiful rooms in her castle. The Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood. ","date":"2019-11-29","objectID":"/syntax/:9:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Nested Blockquotes Blockquotes can be nested. Add a » in front of the paragraph you want to nest. \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. The rendered output looks like this: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-11-29","objectID":"/syntax/:9:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Blockquotes with Other Elements Blockquotes can contain other Markdown formatted elements. Not all elements can be used — you’ll need to experiment to see which ones work. \u003e **The quarterly results look great!** \u003e \u003e - Revenue was off the chart. \u003e - Profits were higher than ever. \u003e \u003e *Everything* is going according to **plan**. The rendered output looks like this: The quarterly results look great! Revenue was off the chart. Profits were higher than ever. Everything is going according to plan. ","date":"2019-11-29","objectID":"/syntax/:9:3","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Lists ","date":"2019-11-29","objectID":"/syntax/:10:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Unordered A list of items in which the order of the items does not explicitly matter. You may use any of the following symbols to denote bullets for each list item: * valid bullet - valid bullet + valid bullet For example: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem The HTML looks like this: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-11-29","objectID":"/syntax/:10:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Ordered A list of items in which the order of items does explicitly matter. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem The HTML looks like this: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e Tip If you just use 1. for each number, Markdown will automatically number each item. For example: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem The rendered output looks like this: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem ","date":"2019-11-29","objectID":"/syntax/:10:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Task Lists Task lists allow you to create a list of items with checkboxes. To create a task list, add dashes (-) and brackets with a space ([ ]) before task list items. To select a checkbox, add an x in between the brackets ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media The rendered output looks like this: Write the press release Update the website Contact the media ","date":"2019-11-29","objectID":"/syntax/:10:3","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Definition Lists Some Markdown processors allow you to create definition lists of terms and their corresponding definitions. To create a definition list, type the term on the first line. On the next line, type a colon followed by a space and the definition. First Term : This is the definition of the first term. Second Term : This is one definition of the second term. : This is another definition of the second term. The HTML looks like this: \u003cdl\u003e \u003cdt\u003eFirst Term\u003c/dt\u003e \u003cdd\u003eThis is the definition of the first term.\u003c/dd\u003e \u003cdt\u003eSecond Term\u003c/dt\u003e \u003cdd\u003eThis is one definition of the second term. \u003c/dd\u003e \u003cdd\u003eThis is another definition of the second term.\u003c/dd\u003e \u003c/dl\u003e The rendered output looks like this: First Term This is the definition of the first term. Second Term This is one definition of the second term. This is another definition of the second term. ","date":"2019-11-29","objectID":"/syntax/:10:4","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Adding Elements in Lists To add another element in a list while preserving the continuity of the list, indent the element four spaces or one tab, as shown in the following examples. Paragraphs * This is the first list item. * Here's the second list item. I need to add another paragraph below the second list item. * And here's the third list item. This is the first list item. Here’s the second list item. I need to add another paragraph below the second list item. And here’s the third list item. Blockquotes * This is the first list item. * Here's the second list item. \u003e A blockquote would look great below the second list item. * And here's the third list item. The rendered output looks like this: This is the first list item. Here’s the second list item. A blockquote would look great below the second list item. And here’s the third list item. ","date":"2019-11-29","objectID":"/syntax/:10:5","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Code ","date":"2019-11-29","objectID":"/syntax/:11:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Inline Code Wrap inline snippets of code with backticks `. In this example, `\u003csection\u003e\u003c/section\u003e` should be wrapped as **code**. The rendered output looks like this: In this example, \u003csection\u003e\u003c/section\u003e should be wrapped as code. The HTML looks like this: \u003cp\u003e In this example, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e should be wrapped with \u003cstrong\u003ecode\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-11-29","objectID":"/syntax/:11:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Indented Code Or indent several lines of code by at least four spaces, as in: // Some comments line 1 of code line 2 of code line 3 of code The rendered output looks like this: // Some comments line 1 of code line 2 of code line 3 of code The HTML looks like this: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-11-29","objectID":"/syntax/:11:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Block Fenced Code Use “fences” ``` to block in multiple lines of code with a language attribute. ```markdown Sample text here... ``` The HTML looks like this: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-11-29","objectID":"/syntax/:11:3","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Syntax Highlighting GFMGitHub Flavored Markdown also supports syntax highlighting. To activate it, simply add the file extension of the language you want to use directly after the first code “fence”, ```js, and syntax highlighting will automatically be applied in the rendered HTML. For example, to apply syntax highlighting to JavaScript code: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` The rendered output looks like this: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; Note Syntax highlighting page in Hugo Docs introduces more about syntax highlighting, including highlight shortcode. ","date":"2019-11-29","objectID":"/syntax/:11:4","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Tables Tables are created by adding pipes as dividers between each cell, and by adding a line of dashes (also separated by bars) beneath the header. Note that the pipes do not need to be vertically aligned. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | The rendered output looks like this: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. The HTML looks like this: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e Right or center aligned text Adding a colon on the right side of the dashes below any heading will right align text for that column. Adding colons on both sides of the dashes below any heading will center align text for that column. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | The rendered output looks like this: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. ","date":"2019-11-29","objectID":"/syntax/:12:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Formatting Text in Tables You can format the text within tables. For example, you can add links, code (words or phrases in backticks (`) only, not code blocks and emphasis. You can’t add headings, blockquotes, lists, horizontal rules, images, or HTML tags. ","date":"2019-11-29","objectID":"/syntax/:12:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Escaping Pipe Characters in Tables You can display a pipe (|) character in a table by using its HTML character code (\u0026#124;). ","date":"2019-11-29","objectID":"/syntax/:12:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Links ","date":"2019-11-29","objectID":"/syntax/:13:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Basic Link To quickly turn a URL or email address into a link, enclose it in angle brackets. To create a link, enclose the link text in brackets (e.g., [Duck Duck Go]) and then follow it immediately with the URL in parentheses (e.g., (https://duckduckgo.com)). \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) The rendered output looks like this (hover over the link, there is no tooltip): https://assemble.io contact@revolunet.com Assemble The HTML looks like this: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-11-29","objectID":"/syntax/:13:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Add a Title [Upstage](https://github.com/upstage/ \"Visit Upstage!\") The rendered output looks like this (hover over the link, there should be a tooltip): Upstage The HTML looks like this: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-11-29","objectID":"/syntax/:13:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Named Anchors Named anchors enable you to jump to the specified anchor point on the same page. For example, each of these chapters: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) will jump to these sections: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. Note The specific placement of the anchor tag seems to be arbitrary. They are placed inline here since it seems to be unobtrusive, and it works. ","date":"2019-11-29","objectID":"/syntax/:13:3","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Formatting Links To emphasize links, add asterisks before and after the brackets and parentheses. To denote links as code, add backticks in the brackets. I love supporting the **[EFF](https://eff.org)**. This is the *[Markdown Guide](https://www.markdownguide.org)*. See the section on [`code`](#code). The rendered output looks like this: I love supporting the EFF. This is the Markdown Guide. See the section on code. The rendered output looks like this: ","date":"2019-11-29","objectID":"/syntax/:13:4","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Automatic URL Linking Many Markdown processors automatically turn URLs into links. That means if you type http://www.example.com, your Markdown processor will automatically turn it into a link even though you haven’t used brackets. http://www.example.com The rendered output looks like this: http://www.example.com ","date":"2019-11-29","objectID":"/syntax/:13:5","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Disabling Automatic URL Linking If you don’t want a URL to be automatically linked, you can remove the link by denoting the URL as code with backticks. `http://www.example.com` The rendered output looks like this: http://www.example.com ","date":"2019-11-29","objectID":"/syntax/:13:6","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Best Practices Markdown applications don’t agree on how to handle spaces in the middle of a URL. For compatibility, try to URL encode any spaces with %20. ✅ Recommended ❌ Avoid [link](https://www.example.com/my%20great%20page) [link](https://www.example.com/my great page) ","date":"2019-11-29","objectID":"/syntax/:13:7","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Images Images have a similar syntax to links but include a preceding exclamation point. ![Minion](https://octodex.github.com/images/ironcat.jpg) or: ![Alt text](https://octodex.github.com/images/xtocat.jpg \"The X-tocat\") The X-tocatAlt text \" The X-tocat Like links, images also have a footnote style syntax: ![Alt text][id] The DojocatAlt text \" The Dojocat With a reference later in the document defining the URL location: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" Who knew there was a way to do gifs as well. ![Daftpunktocat-Guy](https://octodex.github.com/images/daftpunktocat-guy.gif \"The Daftpunktocat Guy\") The Daftpunktocat GuyDaftpunktocat-Guy \" The Daftpunktocat Guy ","date":"2019-11-29","objectID":"/syntax/:14:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Emoji There are two ways to add emoji to Markdown files: copy and paste the emoji into your Markdown-formatted text, or type emoji shortcodes. ","date":"2019-11-29","objectID":"/syntax/:15:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Copying and Pasting Emoji In most cases, you can simply copy an emoji from a source like Emojipedia and paste it into your document. Many Markdown applications will automatically display the emoji in the Markdown-formatted text. The HTML and PDF files you export from your Markdown application should display the emoji. Tip If you’re using a static site generator, make sure you encode HTML pages as UTF-8. ","date":"2019-11-29","objectID":"/syntax/:15:1","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Using Emoji Shortcodes Some Markdown applications allow you to insert emoji by typing emoji shortcodes. These begin and end with a colon and include the name of an emoji. Gone camping! ⛺ Be back soon. That is so funny! 😂 The rendered output looks like this: Gone camping! ⛺ Be back soon. That is so funny! 😂 Note You can use this list of emoji shortcodes, but keep in mind that emoji shortcodes vary from application to application Refer to your Markdown application’s documentation for more information. ","date":"2019-11-29","objectID":"/syntax/:15:2","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["Markdown"],"content":"Footnotes Footnotes allow you to add notes and references without cluttering the body of the document. When you create a footnote, a superscript number with a link appears where you added the footnote reference. Readers can click the link to jump to the content of the footnote at the bottom of the page. To create a footnote reference, add a caret and an identifier inside brackets ([^1]). Identifiers can be numbers or words, but they can’t contain spaces or tabs. Identifiers only correlate the footnote reference with the footnote itself — in the output, footnotes are numbered sequentially. Add the footnote using another caret and number inside brackets with a colon and text ([^1]: My footnote.). You don’t have to put footnotes at the end of the document. You can put them anywhere except inside other elements like lists, block quotes, and tables. This is a digital footnote[^1]. This is a footnote with \"label\"[^label] and here's a longer one.[^bignote] [^1]: This is a digital footnote [^label]: This is a footnote with \"label\" [^bignote]: Here's one with multiple paragraphs and code. Indent paragraphs to include them in the footnote. `{ my code }` Add as many paragraphs as you like. The rendered output looks like this: This is a digital footnote1. This is a footnote with “label”2 and here’s a longer one.3 This is a digital footnote ↩︎ This is a footnote with “label” ↩︎ Here’s one with multiple paragraphs and code. Indent paragraphs to include them in the footnote. { my code } Add as many paragraphs as you like. ↩︎ ","date":"2019-11-29","objectID":"/syntax/:16:0","tags":["markdown","html"],"title":"Ultimate Guide to Markdown","uri":"/syntax/"},{"categories":["search"],"content":"In the previous articles of this advent series, we described the architecture of our search engine and highlighted technologies which help us serve search results. After many iterations on our architecture, we are finally on a microservice architecture pattern, driven through a container orchestration platform: Kubernetes. This brought us many engineering improvements, but it also has introduced new challenges, such as, the ability to perform a dataset propogation via Pub-Sub, along with volume management for microservices. Let us illustrate this point further by example. ","date":"2019-12-15","objectID":"/hydra/:0:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Scenario—Working with Services and Datasets Let us say, a team of engineers are developing a service, which has a dependency on dataset X, Y and Z. The service requires that periodically, these datasets are updated (Data Updates). Typically, a dataset will be versioned. Hence, there is the need for a versioning scheme, given the frequency of updates, time-stamps are a good option. e.g.: 2019-12-15T00-00-00 i.e. YYYY-MM-DDThh-mm-ss (Data Versioning). For the service to present a holistic view of the system the datasets in a given version can have interdependencies (Service-Data State Management). Newer datasets can be published periodically through an automated pipeline and artifacts stored in an object store like S3. One then needs to keep track of when these datasets finish building and thereby notify the downstream service of their availability (keeping in mind that all interdependencies must be satisfied). Once all three datasets are available, we push the data to instances where services are running (Data provisioning on Volumes). The data is downloaded behind the scenes, without affecting the downstream service. Eventually, the service gets notified of successful download for all the datasets on the instance. Service intrinsically identifies and verifies this change and shifts to new datasets incrementally avoiding downtime.. This is a very common pattern for services, which rely on frequently updating immutable datasets. If this is a one-off scenario, for instance a prototype or a service whose data updates once every 3 months, then you are probably better off doing all steps necessary by hand or by writing a custom automation scripts. But, if this is a regular scenario like in the real world—for example when dealing with multiple services, with a multitude of datasets, each having their own periodicity—it may lead to potentially severe issues pretty quickly. Moreover, there can be two major failure scenarios: Pods can be evicted (Kubernetes Specific). Instances may be lost (General Scenario). Hence, centrally automating dataset propagation, in a high failure system has its challenges but brings in considerable value. In this post, we will briefly introduce a solution built in-house, named Hydra, which provides a generic solution to the use-case mentioned above. We describe the need for such a system in our organization. On a high level, Hydra is composed of a set of components which, Provide a dataset pub-sub mechanism. Maintain a global state. Manage datasets on volumes for services running within a Kubernetes cluster. At its core, Hydra is able to tap into the Kubernetes service registry for discovery and observability of running services. ","date":"2019-12-15","objectID":"/hydra/:1:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Managing lifecycle of datasets (models and indexes) on Instances One of the core requirements of our search engine is fast access to different types of datasets which can have both static or dynamic composition. The dynamism of datasets can be described w.r.t their update frequency (some update real-time, some based on a triggered event and some based on a pre-configured cron job setting). The dataset representation can be a machine learned model (Tensorflow, Keras, Scikit-Learn, etc.), a Granne Index (Approximate Nearest Neighbor Index), qpick Index, a key-value compiled dataset (Keyvi, RocksDB, etc.), a service configuration, or an artifact from a batch job. The size may range from a couple of megabytes to several terabytes. For the purpose of serving search results, the medium and placement of these indexes and models is also important. Datasets may be located on spinning magnetic disks, SSDs or on RAM (ram-disks). This placement differs from service to service, with varying degree of Service Level Objectives (SLO) requirements (access patterns, latency guarantees, replication, etc.) In the context of cloud provisioning, the storage medium can also differ. For some models which do not have strict latency requirements, we choose Elastic Block Storage (EBS) but for others where read latency needs to be minimized and IOPS can be a determining factor to achieve SLOs, we employ local volumes based on raided SSDs using the NVME protocol. For services requiring even more guarantees we rely on large ram-disks. Every microservice in the critical request path, as explained in our previous article1, can have a persistent layer composed of the aforementioned datasets. In order to provide fresh results and to support graceful degradation on failures we need to have a solid mechanism for dataset propagation. Other use cases may include: data exploration while developing new features, running A/B tests or modifying configuration of running services on the fly. Finally, there is another important reason for solid dataset propagation. If it does not exist, every team working on a feature-related data will have to reinvent the wheel. We have experienced first hand that multiple teams come up with different solutions for downloading and keeping state of their services, this tooling, however, was tightly coupled with the system, making re-usability impossible. A common system is needed to save people the tedious task of “getting ready”. ","date":"2019-12-15","objectID":"/hydra/:2:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Introducing Hydra These requirements presented an opportunity to centralize said effort and bake it with infrastructure support. A centralized effort also brings in an opportunity to build better observability and fault detection tools. Knowing the actual placement of the data as well as the ability to quickly change it, is vital for a working system, where the data is served from and facilitates quick changes. The service which solves these challenges for us is called Hydra. The core constituents of Hydra are as follows: It provides a generic pub-sub approach to handling datasets at scale and aids a service to publish or subscribe to versioned datasets. Introduces a concept of channels, publishers and consumers. A channel contains an ordered list of releases where the head pointer points to the latest release subscribers (services) would be interested in. Channel states are managed by a Global Manager and metadata for the same is maintained in a highly available Consul. Publishers can update a channel with a new release (HTTP API) and consumers will be notified of new changes. Data update diffs are propagated from Global Manager to downstream agents named Docto running as deamon-sets which can ensure a global state (downloading of data from S3 and cleanup) in a reconciliation loop. On successful availability of a release on an instance, consumers are incrementally notified of newer datasets, which are then rolled out. This is done via an additional component named AppMan which is responsible for this task. This allows services to be agnostic of Hydra’s workings. Newer datasets can be rolled out using the readiness probe feature from Kubernetes. Whereby given a service is in high availability, rolling data deployments can be done by draining traffic from a pod using readiness probes. In scenarios of replication, this is done one by one for a given service, thereby reducing downtime. The datasets are immutable in nature i.e. a single release represents a complete view of data and incurs no dependency on previous versioned data releases in the same channel. ","date":"2019-12-15","objectID":"/hydra/:3:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Working with Hydra—End-to-End Use Case ","date":"2019-12-15","objectID":"/hydra/:4:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Publishing State of a Release A user or a service can post the dataset source paths (composed within a release) using client-side libraries / CLI and its storage placement as required with the help of a data template, provided as part of Hydra’s client CLI tool. This complete information is posted via the HTTP API to a Global Manager Server. DISK = 0 RAM = 1 DATA_TEMPLATE = { \"dataset_1\": { \"SourcePath\": f\"s3://some-bucket/some-data-set-1/{VERSION_PLACEHOLDER}/file1.xyz\", \"Placement\": DISK }, \"dataset_2\": { \"SourcePath\": f\"s3://some-bucket/some-data-set-2/{VERSION_PLACEHOLDER}/file2.xyz\", \"Placement\": RAM }, } Where, SourcePath is the original S3 bucket path where the data is / will be available, and Placement describes where the data must be eventually provisioned on the instance. Some extra information like Environment, Release and Namespace are also required by the Hydra CLI client. It allows Hydra to formulate channel identity and scoping and is stored as metadata for respective channel. This can be propagated using special flags: --env: Environment describes which cluster to target --release: Release name of service relevant to dataset template -n: Namespace of deployed service The hydra client submits this data template with the VERSION_PLACEHOLDER computed based on two factors: Identifying most recent datasets available on S3. If datasets should have the same timestamps, then it verifies the same before proceeding ahead. We assume that once datasets are available on S3 they have been already verified for integrity and are fit to serve. These checks are done as part of the data-pipeline’s post processing step. Channels can have retention policies on the number of releases to keep. Also, you can configure the number of actual releases to keep on instances. The default value is 2. This means that apart from the newly published release, the previous dataset is also preserved on instances. All older releases can be cleaned up based on retention policies to improve disk utilization. ","date":"2019-12-15","objectID":"/hydra/:4:1","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Hydra’s Global Manager \u0026 Data Download (S3 -\u003e Instance) Once a new data release is published to a channel, the global manager is able to correlate consumers based on discovery from the Kubernetes service registry or explicit registration. All the instances in the cluster run an agent as a daemon-set which only communicates with the Global manager and is wholly responsible to download the models from S3 to instance. Thus the global manager communicates with this daemon application and sends information about which datasets to download. The daemon verifies the requests, identifies the placement, checks if there is enough storage available on the instance and starts the download. It constantly communicates the progress of the download with Global Manager and notifies a SUCCESS upon its completion. ","date":"2019-12-15","objectID":"/hydra/:4:2","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Application Dataset Reload Next, the Global Manager notifies the consumer through client-side library interfaces on availability of newer release datasets. This is done incrementally to avoid downtimes. This initiates the process, where at first readiness probes are used to drain traffic, then either by using a built-in mechanism or a service restart, new data is reloaded for the service, and readiness probes are turned back ON, to serve live traffic. If there are several replica pods behind a service, the Global manager performs the reload one by one, making sure each pod comes back up before starting the data reloads in other pods of the same service. Notes: In development settings, for non critical data services we employ a downscaler strategy which downscales deployments to zero during non working hours. This constitutes an uptime during the following times: Mon-Fri 08:00-19:30 Europe/Berlin. This means that not only the container, but instance with data is downscaled. But, since hydra is autonomous, we can quickly prepare the instance with the data when the deployment is scaled up again on a newer instance. This works great for us, as we save some costs by not spinning high capacity instances (a necessary requirement for hosting search indexes) in development setting during non-working hours. In a special case where RAM is the preferred placement medium, given the fact that they are usually a scarce and expensive resource, we make sure that they are also copied over to disk if available for faster load times to RAMDISK. We only keep a single version on RAM for that dataset. Due to locking on older datasets or limited availability of space on-ramdisk services can be drained and shifted to newer releases. ","date":"2019-12-15","objectID":"/hydra/:4:3","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Future Work Hydra shares a lot of its DNA with project Gutenberg, which was announced two months ago. We will keep an eye on Gutenberg’s development since both systems attempt to solve the very same problems. It is just natural that different teams reach similar conclusions when presented with the same problem. Nonetheless, it is always nice to get external validation of the decision taken. We have been developing Hydra for the last 2 years and gradually integrating it into our production systems. However, it is far from a finished project. A peak on the project’s future timeline: Better control over Volume discovery, provisioning, budgeting and isolation. Multi language support for client side libraries. Including Rust and Go. Currently we only provide Python. More scoping options such as region and pinning releases to a specific version. Encryption and access control to improve security footprint and avoid disaster scenarios. Better incremental rollouts by introducing new strategies like disruption budgets and surge protections. Improvements in observability for service owners. Better channel retention strategies, such as time based retention. ","date":"2019-12-15","objectID":"/hydra/:5:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Conclusion At Cliqz, Hydra runs in production and handles data for a variety of data-hungry services deployed in our search stack. It drives part of our search index which is updated weekly (window-based) as part of our multi-tier lambda architecture1. Through this post, we wanted to highlight the common challenges in data management and how we solve them using Hydra. We plan to share more details in future posts. Hydra is still under active development and will eventually be open sourced. We believe, that the Kubernetes and ML community can greatly benefit from an end-to-end model management versioning and propagation system. ","date":"2019-12-15","objectID":"/hydra/:6:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"Footnotes and Remarks The Architecture of a Large-Scale Web Search Engine, circa 2019. ↩︎ ","date":"2019-12-15","objectID":"/hydra/:7:0","tags":["machine learning","model management","data versioning","pub sub","kubernetes"],"title":"Hydra—Kubernetes based Dataset PubSub and Volume Management System","uri":"/hydra/"},{"categories":["search"],"content":"In previous posts of this series, we have described some of the technologies that power our private search products. It is about time that we introduce the systems that bring everything together. It is important to understand that a web scale search engine is highly complex. It is a distributed system with strong constraints on performance and latency. On top of that it can easily become extremely costly to operate; both in human resource and, of course, in money. This article explores the technology stack we employ today and some of our choices and decisions, which have been taken and iterated upon over the years, to cater both external and internal users. The topic at hand is very broad and cannot be covered in a single sitting, but we hope to give you the gist of it. We use a combination of prominent open source and cloud-native technologies wrapped with home grown tooling, which have been battle tested. Places where we haven’t found a solution in the open source world or commercial efforts, we have been prepared to dive deep and write some core systems from scratch, which has worked well for us at our scale. Disclaimer: We describe how our system is, as of today. Of course we did not start like this. We had multiple architectural overhauls throughout the years, always considering constraints like costs, traffic and data size. By no means, we would suggest that this is a recipe to build a search engine; it is what is working today, as wiser people said: “Premature optimization is the root of all evil” ~ Donald Knuth And we agree wholeheartedly. As matter of fact, we really advise anyone, to never try to throw all the ingredients to the pot at once. But instead to add them one by one; slowly and incrementally adding complexity one step at a time. Given the nature of this post, we want to provide an ordered outline of all topics covered: Cliqz search as a product and its system requirements. Web Search Systems: A near real-time and truly automated search system. Data Processing Platform: Facilitating near Real-time and Batch Indexing. How deployments were done in the past? The Pros and Cons of various approaches. Microservices Architecture: Orchestrating services involved to deliver content for a search engine result page. Our need for using containers and a container orchestration system (Kubernetes). Introduce our Kubernetes stack - How we deploy, run and manage Kubernetes and various add-ons and the problems they solve for us. Local Development on Kubernetes - An end to end use case. Optimizing on Costs. Machine Learning Systems. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:0:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Our Search Experience - Dropdown \u0026 SERP The search engine at Cliqz has two consumers with different requirements. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:1:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Search-as-you-type The search in the browser address bar1, with results available on the dropdown. This type of search requires fewer results (typically 3) but is extremely latency sensitive (less than 150 ms); otherwise the user experience suffers. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:1:1","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Search in SERP Search on a web page, the typical search engine results page everybody knows. In here, the depth of the search is unbounded but it is less demanding on latency (less than 1000 ms) as compared to the dropdown version. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:1:2","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Fully Automated and Near Real-time Search Consider a query like “bayern munich”. Now, this may seem a very generic query, but when issued, it touches several services within our system. If we try to interpret the intent from the query, we will figure out that the user may be: Researching about the club (in which case a Wikipedia snippet would be relevant) Interested in booking tickets, buy merchandise, register as an official fan (Official Website) Interested in current news about the club: Pre-Match news about the game In-game information like: Live Scores, Live Updates or Commentary. Post-match analysis about the game Off-season Information about the inner workings of the club and activity during the transfer window, hiring new coaches etc. Searching for old web-pages and content, club history, record of past games, etc. As one might observe, this is much more than finding relevant pages. Not only should the information requested be semantically relevant, but it should be relevant w.r.t time. Recency or Temporal Sensitivity in search is a very important factor for the user experience. To make this a coherent experience, the information must be made available from different sources and transformed into a search-able index in near real-time. We need to ensure that all models, indexes and assets are up-to-date (e.g. Loading of images must reflect the current events and keep the title and content up-to-date with respect to a developing story). As hard as it may seem to execute successfully at scale, we strongly believe that our users should always be presented with up-to-date information, this intuition forms the basis of our overall system architecture. The data processing and serving platform at Cliqz follows a multi-tiered Lambda Architecture. It is composed of three tiers based on the recency of the content being indexed. They are: Near Real-time Indexing Fully automated and powered using Kafka (Producer, Consumers and Stream Processors), Cassandra, Granne2 and RocksDB. Cassandra stores the Index data in different tables. Records in different table have varying Time to Live (TTL), so we can free up the storage as the data is re-indexed in later stages. This component is also responsible for our trending or popularity driven ranking features which help identify trends over a moving window of varying time sizes. We accomplish this via stream processing using KafkaStreams. All this translates into product features including recent content within search results and top news. Weekly or Sliding-Window-Based Batch Indexing Based on content of past 60 days. Re-indexed weekly (End to End automated pipelines of batch jobs on Jenkins). Machine Learning and Data Pipelines are executed upon recent data resulting in higher quality of our search results. A good framework to test and prototype new ML models and algorithmic changes using a subset of data thereby reducing costs of end to end experiments on entire data. Map-Reduce and Spark based batch workflows managed through Luigi and retrospectively managed using Jenkins Pipelines. Keyvi, Cassandra, qpick and Granne for serving. Full Batch Index Based on all available data Re-Indexing: once every 2 months MapReduce and Spark based batch workflows managed through Luigi Used to train large scale Machine Learning Models over a large data-set. e.g.: Query and Word Embeddings, Approximate Nearest Neighbour Models, Language Models etc. Keyvi, Cassandra, qpick and Granne for serving What is important to note here is that, Near Real-time and Weekly Index is responsible for a large portion of search related content served on SERP. This is a similar behavior to other search engines which promote recent content over historical content about the topic. The batch index is handling time independent queries, long tail of queries and content which is rare, historical or tricky in the context of understanding a search query. The combination of the three gives us the necessary ammunition to build Cliqz sear","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:2:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Deployments - A Historical Context You haven’t mastered a tool until you understand when it should not be used. ~ Kelsey Hightower From the start, we have been focused on delivering our search services using a public cloud provider rather than managing infrastructure on-premises. In the last decade, that has become the norm across the industry, given the complexity and resources required to operate one’s own data center(s) compared with the relative ease of hosted services and the ease to digest pay as you go model for startups. Amazon Web Services (AWS)3 has been convenient for us as it allowed to abstract ourselves from managing our own machines and infrastructure. If not for AWS, it would have taken us a lot more effort to reach this stage. (But, they are as convenient as they are expensive. You will see in this article some tricks we came up with to reduce costs, but we advise you to be extremely careful using cloud services at scale.) We typically strive to avoid a managed offering of a service that might be useful to us as costs can be unbearably high at the scale we typically operate at. To bring in some context let us start from the year 2014. A growing concern that we met early on was reliably provisioning resources and deploying applications on AWS. We started with putting some effort into building our own infrastructure and configuration management systems on top of AWS. We focused on shipping a solution which was native to python to ease developer on-boarding. We wrapped the Fabric project and coupled it with Boto to provide nice interfaces to launch machines and configure one’s application with few lines of code driven through a deploy.py file co-located to the service source. This was then wrapped into project template generators for easy onboarding of new projects. Back then, it was early days of docker and traditionally we shipped as python packages or plain python code which was challenging because of dependency management. Even though the project gained a lot of traction and was used by many services driving many products at Cliqz, there are certainly things a library driven approach to infrastructure and configuration management lacks. Global state management, central locking for infra changes, no central view of cloud resources utilized by a project or developer, reliance on external tools to cleanup orphaned resources, limited configuration management, limited observability into developer usage, context seep in from regular users of the tool are some of the things that created friction and which in turn led to an increased operational complexity. This led us to explore alternate external solutions as homegrown efforts had to be stopped due to limited resources. The alternative we eventually landed on is a combination of solutions from Hashicorp including Consul, Terraform and Packer and eventually configuration management tooling like Ansible and Salt. Terraform presented an excellent declarative approach to infrastructure management which many of the current technologies in the cloud native space have leveraged. So we decided, after careful evaluation, to retire our fab-based deploy library for Terraform. Besides technical pros and cons, one always have to consider human factors. Some teams are slower to adopt changes than other, be it because of lack of resources or because the cost of transitions are not uniform. For us, it took a long time, about one year, to migrate. Terraform certainly brought us some out of the box features which we were missing from our old deploy project, including: Central state management of infrastructure. Verbose plan, patch and apply support. Easy teardown of resources with minimal orphaned resources. Support for multiple clouds. Meanwhile, we also faced some challenges in our journey with Terraform: Complex cloud specific DSL which is typically not DRY. Difficult to wrap it in other tools. Limited and sometimes complex templating support. No feedback on the health of service","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:3:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Intricacies of a Search System Over the years, we have moved from a distributed architecture, with dozens of servers, to a monolithic architecture, to finally end up on a microservices architecture. Each solution we believed in was the most convenient at that time, given the resources available; for instance, the monolith version was due to the fact that most of our latency came out of network IO among the machines of the cluster. At that time, AWS launched the X1 Instance, with a whopping 2 TB of RAM. A quick change of architecture allowed us to reduce latency quickly, but of course, costs went up. The next iteration on architecture we focused ourselves on cost. Step by step we tried to fix one variable without worsening the others. It might not look like a very fancy approach, but it worked well for us. The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. ~Martin Fowler The microservice definition by Martin Fowler is technically correct, but is somewhat abstract. To us it generally does not give enough of a description on how to exactly build and proportion your microservices, which are important concerns. The move to microservices brought us: Better modularity and autonomy of teams and separation of concerns. Horizontal scalability and workload partitioning. Fault isolation and better support for multiple languages. Multi-tenancy and generally a better security footprint. Better automation in operations. Looking at a broad picture of the architecture and how we structure our microservices when a search query is issued to the backend, a number of services are triggered in the request path. Each service is considered a microservice in the sense it has separation of concern, is driven by a lightweight protocol (REST/ GRPC) and is horizontally scalable. Each service can be constituted of individual microservices and can have a persistence layer. The request path typically involves: Web application firewall (WAF) - Application firewall against common web exploits. Load Balancers - Request ingestion and load balancing. Ingress proxies - Routing, edge observability, discovery, policy enforcement. Eagle - Server side rendering for SERP. Fuse - API Gateway, results mixer, edge caching, authentication / authorization. Suggest - Query Suggestions4. Ranking5 - Serves search results using a near real-time index and pre-compiled batch index (Lambda Architecture). Rich Results6 - Adds rich information like snippets for weather, live scores, other third party sources. Knowledge Graph and Instant Answers - To the point information related to a query. Places - Geo-Local based content recommendation. News - Real-time Content from reputable news sources. Tracker - Domain specific tracker Information via WhoTracks.me. Images - Image results relevant to user query. All these services are orchestrated through a common API gateway responsible for handling search volume as well equipped with features like providing protection around traffic surges, Auto-scaling based on requests / cpu / memory / custom metrics usage, edge caching, traffic shadowing and splitting, A/B Testing7, Blue-Green8 deployments, Canary release9, etc. The platform is also responsible for providing many of these functionalities to services in play. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:4:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Docker Containers and Container Orchestration System So far, we have described some of the requirements and specific details about our product offering. We described how we did deployments and what were the shortcomings with the solutions which we tried. Given these learnings, we chose Docker, as the fundamental building block for all of our services. We started delivering our code using docker containers, instead of using plain VMs with code and dependencies. With docker, code and its dependencies are shipped as docker images to a container registry (ECR). But once the services started growing again, there was a need to manage them, specially when we wanted to scale them in production. It became apparent over the years that we had to introduce a container orchestration system. The pain points that led to the introduction were typically wasted compute resources and complexities in infrastructure and configuration management. We are always short on people and on computing power, this is a situation shared by most start-ups with limited resources. Of course, to be effective, we must focus on solving the problems that we have, that cannot be solved by tools that already exists. But, we do not believe in reinventing the wheel (until it radically changes the existing landscape). We are avid users of open source software, where we found solutions to our critical business problems. We started evaluating Kubernetes10 (k8s) as soon as the 1.0 version was released and had production level workloads running by 1.4, once the project showed stability and maturity in tooling. During the same time, we evaluated other orchestration systems including Apache Mesos and Docker Swarm for our large scale projects like fetcher (web scale crawler). We eventually landed on running everything with Kubernetes, as it was quite evident that the project showed promise in having a cohesive approach to tackling orchestration and configuration management while others didn’t, and also included a strong community support. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:5:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Kubernetes - The Cliqz Stack Open Source has won! Cliqz relies heavily on a lot of Open Source Software (OSS) projects typically under the umbrella of Cloud Native Computing Foundation11 to provide a cohesive cloud native experience. We try our best to contribute back to the community both in terms of code, blog posts and on other channels including slack. We will share details about the usage of some critical OSS projects which form the core of our stack: ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"KOPS - Kubernetes Orchestration For container orchestration, we self manage multi-region Kubernetes clusters using KOPS and some homegrown tooling on top to manage cluster lifecycle and addon management. Shoutout to Justin Santa Barbara and kops maintainers for their awesome work, providing a well integrated experience on bringing up the k8s control plane and worker nodes in a conformant way. Currently we don’t rely on a managed offering just because of the flexibility of KOPS and lack of maturity in EKS, an AWS managed k8s control plane. Using KOPS and self managing a cluster meant that we can really set at our own pace, dive deeper during troubleshooting, activate features which were application requirements but available only in specific Kubernetes releases. If we had waited for a cloud offering, we would had to wait a considerably longer time to reach at the stage that we have now. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:1","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Weave Net - Network Overlay Its important to note that Kubernetes helps to abstract all parts of the system. This not only includes compute and storage, but also networking. For our clusters which can grow to hundreds of nodes, we employ an overlay network which forms the backbone for providing flat networking and network policy enforcement capabilities for pods spawning over multiple nodes, availability zones and regions. Weave Net is the overlay of choice we landed on early because of easy manageability and route management through gossip. As we grow bigger we might shift to AWS VPC CNI and Calico as they mature to provide less network hops and more consistency in routing and traffic. Till now weave net has performed exemplary in our latency and throughput targets and there hasn’t been a reason to switch. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:2","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Helm / Helmfile - Package management and delivery We have relied on helm (v2) from the start for package and release management of Kubernetes manifests. Even though it has its pain points, we found it to be an excellent resource for release management and templating. We follow a single repository structure for helm charts of all of our services which are packaged and served using the chartmuseum project. Environment specific values are then kept in a separate repository to separate concerns. These are driven using the gitOps pattern through Helmfile, which provides a declarative approach to multiple helm charts release management and associated essential plugins like diff, tillerless and secret management at rest with SOPS. Changes to this repository are validated and deployed using CI / CD pipelines driven through Jenkins. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:3","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Tilt / K9s - No stress local Kubernetes development One of the problems we faced early on was: How best to include k8s into the inner loop of a developer development life-cycle. Some requirements are quickly apparent. How to build and sync code into containers and do it as fast as possible. Initially we had a simple homegrown solution to tap into filesystem events on source changes and just rsync everything to containers. We also experimented with projects including Skaffold from Google and Draft from Microsoft trying to solve the same problems. Eventually what worked for us is Tilt from Windmill Engineering (shoutout to Daniel Bentley), who have an excellent product on their hands with a workflow driven by Tiltfile written using starlark language. It is able to watch files for edits, can apply changes automatically, build container images in real-time, make build faster with in cluster builds and skip registries and has a nice UI to see all information about your services in a single pane, etc. And when you want to dig a bit deeper we open the nitty gritty of k8s through an awesome CLI tool called K9s to interactively run k9s commands and simplify developer workflows. Today all of our workloads running in k8s are developed in cluster with a cohesive and fast experience across projects thanks to helm / tilt / k9s and anyone new joining in can easily onboard with a few commands. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:4","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Prometheus, AlertManager, Jaeger, Grafana and Loki - Observability We rely heavily on the Prometheus monitoring solution and time series database (tsdb) to gather, aggregate and forward our metrics scrapped from individual services. Prometheus ships with an excellent querying language PromQl and alerting solution Alert Manager. Jaeger forms the backbone for our trace aggregation. Recently we started shifting from Graylog to Loki as our logging backend to provide a similar experience as Prometheus. This all is done to provide a single pane of glass where all observability requirements can be met and we intend to deliver that with Grafana which is our charting solution. To orchestrate all these services we rely heavily on the Prometheus Operator project with integrations on top to manage the life-cycle of many multi-tenant Prometheus deployments that we have today. At any given time we ingest hundreds of thousands of time series to gather insights about running infrastructure and services to aid when something fails. In the future, we plan to either integrate Thanos or Cortex project to address Prometheus scalability challenges and provide a global query view, high availability and data backup for historical analysis. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:5","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Luigi and Jenkins - Automating Data-pipelines We use Luigi and Jenkins to orchestrate and automate our data pipelines. The batch jobs are submitted via steps to EMR and Luigi helps us to build highly complex batch workflows on top of these jobs. Jenkins is utilized to trigger a series of operations in course of an ETL process providing us control over automation and use of resources, drilled down to individual task. We package and version our batch job code in versioned docker containers, to keep a consistent developer and production experience. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:6","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Addon Projects We also include many other projects developed by the community, which are delivered as addons and maintained as part of the cluster life-cycle to provide value to the services deployed both in production and development environments. We discuss them briefly below: Argo Workflow and CD - Evaluating as an alternative to Jenkins for batch processing task and continuous deployment efforts. AWS IAM Authenticator - User identity management in k8s. ChartMuseum - Serves our remote helm charts. Cluster Autoscaler - Manages scaling of on-demand and spot fleets in our clusters. Vertical Pod Autoscaler - Vertically scales pods where necessary or based on custom metrics. Consul - State store for many projects, where we can we try to move to Custom Resource Definitions (CRDs). External DNS - Maps DNS records to Route53 for external and internal access. Kube Downscaler - Downscales deployments and statefulsets when they are no longer in use. Kube2IAM - Transparent proxy to restrict aws metadata access and role management for pods. Loki / Promtail - Log shipment and aggregation. Metrics Server - Metrics aggregation and interfacing for other consumers. Nginx Ingress - Ingress controller for internal and external services. We continue to evaluate other ingress controllers for extended API gateway functionalities including Gloo, Istio ingress gateway and Kong. Prometheus Operator - Prometheus operator stack to provision Grafana, Prometheus, AlertManager and Jaeger Deployments. RBAC Manager - An operator to easily manage Role Based Access Control for k8s resources. Spot Termination Handler - Handles spot termination gracefully by preemptively cordoning and draining nodes. Istio - We continue to evaluate Istio for its mesh, observability, traffic routing and shaping capabilities. For many of these features, we have built solutions in house, which have shown limitations over time, which we intend to fulfill with this excellent project. Our experience with k8s coupled with excellent community tooling has enabled us to not only ship our core stateless services providing search, but also helped us run large and critical stateful workloads like Cassandra, Kafka, Memcached and RocksDB in multiple availability zones and clusters, for high availability and replication. We have developed additional tooling to manage and safely execute these workload for our scale within Kubernetes. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:6:7","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Local Development with Tilt - An end to end use case So far we covered a lot of ground describing all the tools we use, but we would like to give a concrete example on how this tooling, well, parts of it, is affecting the typical day-to-day workflow of our developers. Let us take the example of an engineer working on Ranking. Previously, the workflow was: Start a spot instance with a custom OS image, tag instance and associated resources with ownership information. Rsync application code to the instance and install application dependencies. Figure out setting up other services like api-gateway and front-end, their dependencies and deployments. Configure them to work well with one another. Start working on the Ranking application. At the end, once done, make sure to terminate the instance. We can see that, one needs to follow a series of steps repeatedly, over and over again and every new engineer in the team will have to repeat it, which is a total waste of developer productivity. If the instance is lost we repeat. Also there is a stark difference between the production and local development workflow, leading to inconsistencies at some point of time. One might also argue the need for setting up other services like front-end along with ranking, but aim is to be generic here and in addition its always good to have complete visibility of the product. Additionally, as the team size grows, more cloud resources are created and more resources are under-utilized. Engineers let instances run, because they don’t want to redo the setup process every day. If a team member leaves and has an orphan instance without sufficient tags, it is a challenge to identify whether or not it is safe to turn off the instance and delete said cloud resources. What would be ideal, is to provide an engineer with a base template to set up local environment with his own full version of SERP along with other services responsible for ranking. We configure the base template generically, which tags resources created by the user with thier unique identifier names and allows them to control the life-cycle of their application. As K8s already abstract out need to launch instances and managing them (we centrally administer them using KOPS), we use the template to set defaults (auto downscaling during non-work hours) and hence tremendously reduce costs. All the user now has to care about is the code written in his local editor and our tooling which is composed of a combination of Docker, Helm and Tilt on top of Kubernetes facilitates this said workflow, working behind the scenes like magic. Here is an example Tiltfile, which describes the services and other dependent services required to setup a mini-SERP version. For launching these services in development mode, all the user has to do is run: tilt up # -*- mode: Python -*- \"\"\" This Tiltfile manages 1 primary service which depends on a number of other micro services. Also, it makes it easier to launch some extra ancilliary services which may be useful during development. Here's a quick rundown of these services and their properties: * ranking: Handles ranking * api-gateway: API Gateway for frontend * frontend: Server Side Rendering for SERP \"\"\" #################### # Project defaults # #################### project = \"some-project\" namespace = \"some-namespace\" chart_name = \"some-project-chart\" deploy_path = \"../../deploy\" charts_path = \"{}/charts\".format(deploy_path) chart_path = \"{}/{}\".format(charts_path, chart_name) values_path = \"{}/some-project/services/development.yaml\".format(deploy_path) secrets_path = \"{}/some-project/services/secrets.yaml\".format(deploy_path) secrets_dec_path = \"{}/some-project/services/secrets.yaml.dec\".format(deploy_path) chart_version = \"X.X.X\" # Load tiltfile library load(\"../../libs/tilt/Tiltfile\", \"validate_environment\") env = validate_environment(project, namespace) # Docker repository path for components serving_image = env[\"docker_registry\"] + \"/some-repo/services/some-project/serving","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:7:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Optimizing Costs Having a cheap infrastructure and a web scale search-engine do not go hand in hand, that said, there are ways by which you can save money. Let’s discuss how we optimized for costs using our K8s based infrastructure: 1 . Spot Instances We heavily relied on AWS spot instances, as using them forced us to build the system for failures. But it was worth it, as they are a lot cheaper as compared to on-demand instances. Be wary though of not falling on the same self-inflicted damage we did. We were so used to those, that sometimes we outbid ourselves, happened more often than what we would have liked. Also, Don’t exhaust high performance machines or you will get in a bidding war with other companies. Lastly, Never use spot GPU instances before a major NLP/ML conference. Mixed Instance Pools with Spot: Not only did we utilize spot instances for one-off jobs, but for service workloads. We came up with a neat strategy, where we create a node pool for running kubernetes resources using mixed instance types (but, similar configurations) which were distributed across multiple Availability Zones. This coupled with Spot Termination Handler allowed us to move our stateless workloads just in time to newly created or spare capacity spot nodes thus saving us from a potentially high downtime. 2 . Sharing CPU and Memory As we are fully committed on Kubernetes, we discuss workload provisioning based on how much CPU or memory is necessary and how many replicas does one service need. In this, if the Request and Limits are equal we get guaranteed performance. Although, if the Request is low but Limit is high, which may be useful in case of sporadic workloads, we could over-provision and maximally utilize the resources on an instance (reduce idle resources on an instance). 3 . Cluster Auto-scaler, Vertical and Horizontal Pod Autoscaler We deploy these to automate launching and downscaling of pods and in turn instances only when the need arises. This means when there is no workload, only the minimum set of instances are up and we don’t need manual intervention to facilitate this. 4 . Deployment downscalers in development environment We use deployment down-scalers to downscale pod replicas to 0 at specific times for all services in development setting. Using an annotation in our application’s kubernetes manifest, we can specify an uptime schedule: annotations:downscaler/uptime:Mon-Fri08:00-19:30Europe/Berlin This means the deployment is downscaled to zero during non-working hours. And in turn the instances are automatically downscaled by the cluster autoscaler since there are no active workloads on the instance. 5 . Cost Assessment and instance recommendations – Long term cost reduction In production, once we identify our resource utilization, we can select the instances which will be used heavily. Instead of on-demand, we could go towards a reserved instance pricing model which requires a minimum of 1 year upfront payment. In turn, the costs are significantly cheaper as compared to running instances on-demand. For kubernetes, there are some solutions like kubecost, which monitors usage over time and based on that recommend additional ways to save costs. It also provides an estimated price for a workload, so one can get a decent idea of the overall cost for deploying a system. One is notified also of the resources which might not be of use anymore like ebs volumes, etc in one single interface. All of these steps result in saving tens and thousands of euros for us annually. For larger organizations with high infrastructure bills, if executed properly, these strategies could easily save millions on average per year. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:8:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Machine Learning Systems It is interesting to note that our Kubernetes journey started in the most unexpected manner. We were exploring the idea to set up an infrastructure, which allows us to run distributed deep learning experiments using Tensorflow. At the time this idea was new and although Tensorflow had shipped distributed training a short while ago, apart from a few handful of very well resourced organizations, only few knew how to run and manage this workload End-to-End specially for large settings. It was also a time when there was no cloud offering which could help solve this problem. We started out using a distributed setup deployed using Terraform, but we soon realized this solution has its limitations if we wish to scale it to all engineers of the organization. At the same time we found some community contributions, where plain Kubernetes manifests were generated via the use of smart jinja templating engine to create a distributed deployment of Deep Learning Training Application (Parameter Server \u0026 Worker Mode) This was our first contact with Kubernetes. In parallel, we started working on building our search to work near real-time, along-side experimenting with recency ranking. It was then, when kubernetes shone the brightest for us and we decided to get stuck in and dive deeper into it. As part of our Machine Learning Systems journey, like with most of the infrastructure described above, our goal was to open it to the entire organization and make it easier for all developers to deploy applications on Kubernetes. We really wanted them to focus more on solving the problems instead of trying to solve infrastructure challenges associated with services. But, from all the gains one gets from applying Machine Learning to their problems we quickly realize that maintaining machine learning systems is a real pain. It goes a lot deeper than just writing ML code or training models. Even for an organization at our scale, we need to address some of these issues. They are described in depth in the paper, “Hidden Technical Debt in Machine Learning Systems”12. It is a good read for anyone thinking of relying and running machine learning systems at scale in production. In our process, we looked at several solutions,e.g.: MLT13 AWS SageMaker14 Kubeflow15 MLFlow16 Among all these, we found Kubeflow the most relevant, feature complete, cost effective and customizable for our needs. We have also penned down some of these reasons17 on the official Kubeflow Blog a while back. Apart from providing us with custom resources like TfJob and PytorchJob to run our training code, one of the benefits of kubeflow has been its out of the box excellent notebook support. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:9:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Kubeflow use-case Many of the features responsible for our near-realtime search ranking utilized notebooks from Kubeflow. An engineer can spin up a notebook in the cluster and directly tap into our data infrastructure (batch and realtime streaming). It made it easier to share notebooks and work on different parts of the code together. Experimentation became easier for engineers as they don’t have to re-setup notebook servers, provide perimissions to access data infrastructure and look into the gory details of deployment, using a simple web interface one can choose what resources they needed for the notebook (cpu, memory and even gpu), allocate an ebs volume and start up a notebook server. Interestingly, some of the experiments, were done on notebooks with as low as 0.5 CPU and 1 GB of RAM. Usually this capacity was readily available in our cluster and we could easily facilitate spawning of such notebooks without starting newer instances. In a different setting, if two engineers from different teams were working, they most likely will start their own instances. This leads to increased costs and under-utilization of resources. In addition, Jobs can be submitted which can then be used to train, validate and serve the model from within the notebook. An interesting project in this regard is Fairing. Kubeflow itself is a very comprehensive initiative and we have only started scratching its surface. More recently, we have also started looking into projects like Katib (hyperparameter tuning for machine learning models), KFServing (Serverless inferencing of Machine Learning models on Kubernetes), Kubeflow Pipelines (End to End Machine Learning Pipelines for Kubeflow) and TFX (Create and manage a production ML pipeline). We already have some prototype around these projects and hope to release them to production in near future. Given all these benefits, we would like to thank whole-heartedly the team behind Kubeflow for this amazing project. … As we grow and rely more on machine learning and its variants, we want the processes surrounding Machine Learning to be streamlined and be more reproducible in general. This is where things like model tracking, model management, data versioning and lineage becomes crucial. To run things consistently at our scale where we apply periodic updates and assessments, we needed a solution around data management for serving models in production, which facilitates hot swapping of models and indexes in our live production services autonomously. To tackle this issue, we built a solution in-house “Hydra” which provides downstream services with the capability of performing a dataset pub-sub. It also ensures volume management for services in a Kubernetes cluster. We will describe this in detail in a future post. ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:9:1","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Final Words Once you’ve found success, your next goal should be helping others do the same. ~ Kelsey Hightower Architecting Cliqz has been challenging and fun at the same time. We believe there is still a long road ahead of us. As with growing development in this space, there are several possibilities and routes to take. Even though Cliqz employs 120+ people, the codebase is effectively developed by thousands of passionate open source developers, distributed globally, devoted to ship high quality code, and build safe and secure software systems for mankind. We would not have reached where we are without them. We would like to thank the open source community from the core of our hearts to be really generous and helpful in providing us with solutions, whenever we were stuck. Through this post, we wished to share our struggles, experiences and solutions for others who might have similar problems and are looking for answers. In the spirit of openness, we too are contributing back to the extent of our scarce resources here. A fully private search offering from Cliqz is our contribution towards a privacy focused web, a daunting but not an impossible task. We invite you to try out our search and download our browsers on desktop and mobile to be part of this mission. And, if you enjoy working on such problems come and join us. Auf Wiedersehen (until we see each other again) ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:10:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["search"],"content":"Remarks and references https://0x65.dev/blog/2019-12-11/the-pivot-that-excited-mozilla-and-google.html ↩︎ https://0x65.dev/blog/2019-12-07/indexing-billions-of-text-vectors.html ↩︎ https://aws.amazon.com ↩︎ https://0x65.dev/blog/2019-12-08/how-do-you-spell-boscodictiasaur.html ↩︎ https://0x65.dev/blog/2019-12-06/building-a-search-engine-from-scratch.html ↩︎ https://0x65.dev/blog/2019-12-09/cliqz-rich-results.html ↩︎ https://en.wikipedia.org/wiki/A/B_testing ↩︎ https://martinfowler.com/bliki/BlueGreenDeployment.html ↩︎ https://martinfowler.com/bliki/CanaryRelease.html ↩︎ https://kubernetes.io ↩︎ https://www.cncf.io/ ↩︎ https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf ↩︎ https://www.intel.ai/mlt-the-keras-of-kubernetes/ ↩︎ https://aws.amazon.com/sagemaker ↩︎ https://www.kubeflow.org ↩︎ https://mlflow.org ↩︎ https://medium.com/kubeflow/why-kubeflow-in-your-infrastructure-56b8fabf1f3e ↩︎ ","date":"2019-12-14","objectID":"/architecture-of-large-scale-web-search-engine/:11:0","tags":["realtime","kubernetes","machine learning","cloud native","oss"],"title":"The Architecture of a Large-Scale Web Search Engine","uri":"/architecture-of-large-scale-web-search-engine/"},{"categories":["kubeflow"],"content":"Why Kubeflow In Your Infrastructure.","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"CEOs and CTOs are being challenged by customers, analysts and investors to define how Artificial Intelligence and Machine Learning will impact their revenues and costs. The leading research and development organizations are quickly migrating to open source machine learning frameworks, especially those that take advantage of the operational and infrastructure efficiencies provided by containers, micro-services and Kubernetes. This trend is demonstrated in a recent 451 Research survey which found that over 70% of enterprise organizations surveyed are using Kubernetes. GitHub has over 95M projects, and Kubernetes and Tensorflow are frequently in the top 10 projects, in terms of contributors, discussions, forks, and reviews. With an ever increasing availability of data and compute power, machine learning is turning out to be a powerful tool to solve various problems and helping achieve state of the art results. In such interesting times, Kubeflow has grown very quickly to be one of the most promising ML toolkits in the cloud native open source world. We at Cliqz (a privacy focussed web browser with built-in web search operational in ~7 countries) are also solving some of the most complex problems around user privacy and web search using self managed Kubernetes (kops) on AWS. Since January 2017, we started our cloud native journey and have been building Web Search solutions using Kubernetes. Since December 2017, the Search-Recency system has been in production, helping us towards near-real time index updates leading to most recent and up-to-date search results. To solve this problem at that scale, we heavily use Machine Learning, Natural Language Processing, Deep Learning and core Information Retrieval techniques which led us to explore Kubeflow. We are currently evaluating Kubeflow as a general alternative to our custom ML workflow. We would like to present some initial assessments and how Kubeflow might work well for one’s k8s infrastructure are highlighted below: ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:0:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Know Thy Users It’s important that the target audience which would be interested in Kubeflow should be looked up closely. Most organizations which have an established infrastructure might be reluctant to even move to kubernetes. For example, it took a good amount of time for most teams to migrate to Terraform based deployments and because of this investment in time, switching to kubernetes is sometimes not appreciated. For a cloud native strategy where Kubernetes is preferred, Kubeflow becomes a good candidate for deploying and working with ML components. This brings to light, the following types of teams who can potentially be interested: A team (within an organization) starting out their cloud native journey with K8s, who might want to leverage the consistency offered by Kubeflow for ML workloads for new projects. A very early stage startup which has started out with K8s as base. Teams interested in ML at Scale and want to ease deployment of existing multiple services and reduce management of resources by switching to kubeflow and k8s. Research Teams / Institutes who want to minimize the complexity of managing an infrastructure for a data scientist or a researcher and instead provide a clean and consistent interface which eases setting things up using a few clicks. *Teams interested in on-premise / multi-cloud deployments where there is no service offered which can provide a consistent experience. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:1:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Consistency in Infrastructure One of the greatest advantage of using a k8s based deployment is the consistency and features offered out of the box. Often times each new service tries to implement the same fundamental requisites: monitoring, health checks, replication etc. Kubeflow provides a native way to extend the same features to an organization’s ML needs. This is particularly useful to augment existing services without rewriting deployments from scratch. Having Kubeflow in the organization means one needs to worry more on the problem at hand and less worry about how to set things up and manage it over time. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:2:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Multiple Use Cases Team is researching a problem which can be solved with an ML technique. They just can focus on the problem and not on the infrastructure. A Jupyter Notebook pinned to a GPU instance or a cluster abstracts this out cleanly. Several Researchers can work on shared notebooks and also use the same data backends instead of copying the data over to individual instances. Road to production for ML projects is simplified. The end to end solution offered in Kubeflow helps to productionize an ML model in the fastest way. This allows a team of researchers to finish testing a model for accuracy on a Jupyter Notebook, Build a continuous data pipeline to keep this model updated via argo and then test production workloads using Serving / Seldon. Katib can be a central solution for hyper parameter tuning across several applications. Hyperparameter optimization is one of the most underappreciated yet most important aspects of machine learning. Katib provides the ground framework to extend this to multiple applications and have a shared view of this tuning with historical data. For example Hyperopt is a python library for such optimizations, but it largely is limited to only the scope of the project. For an organization where multiple teams and services are backed by ML they can leverage the common interface which Katib provides to learn more complex but powerful optimizations which can significantly impact the product at large. Also having an infrastructure leads to more teams trying out implementing some solution which can leverage the benefits offered via such optimization. With multiple frameworks being supported (Tensorflow, PyTorch and Mxnet), writing a distributed training or serving application ((TFServing)[https://www.tensorflow.org/serving/] or Seldon) becomes a lot more easier. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:3:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"On-boarding ease It becomes easier to onboard a new developer and a researcher to introduce him to a single cloud independent platform. One can provide templates for deployment based on tasks, which can be easily scheduled on low cost infrastructure as compared to starting instances for test applications. Even for ML workloads, the researcher or a research engineer can abstract the use-case effectively without worrying about underlying cloud deployment. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:4:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Secure and Better control over Infrastructure In an organization, moving towards K8s, helps to standardize some processes. Not only one can make the infrastructure more secure, but can also achieve better control over the same. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:5:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Missing Features We would also like to highlight some requested features (currently missing) for an enterprise rollout. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:6:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Billing of cloud resources per pod / service Currently, when using a cloud provider one is billed per instance or resource used. If the resources are properly tagged the billing works perfectly. Most teams and organizations use this information for cost assessments per project or per team and also view this as an opportunity to minimize costs wherever possible. In a Kubernetes based world, this slightly changes. Its highly likely that on the same base instance a number of other pods are scheduled and this may mean that the resources are shared by teams or projects. Now this helps with resource utilization but it also means we do not have a view of the resource usage per project. In ideal world, teams can be charged on average, but this may not work in practice as its highly likely the resource utilization varies. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:7:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Conformance with other package managers (Ex. Helm) We use Helm for almost all our package management needs. With features like templating, release management, upgrades and rollback. It forms the backbone of how we deploy our services and even core addons on our k8s cluster. We operate multiple clusters, spread across various regions / availability zones, housing production, staging and dev workloads. Helm allows us to schedule workloads on these clusters seamlessly by utilizing its decent templating support. When working with Kubeflow, Helm is not supported by default. Although, ksonnet is feature rich, using it means that we break our tried and tested helm workflow. The rationale of choosing helm was its popularity (CNCF Project, 7800+ Stars, 382 Contributors), incredible community support and public chart store which provides a good starting point to install packages into K8s cluster with ease. This speeds up initial development, as one can easily modify charts / inject values and get an enterprise grade deployment ready in limited time, geared towards ones organisation practices. We believe that a lot of teams using k8s might have the same mindset and a proper helm support for Kubeflow might be of interest to many. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:8:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Resource utilization by the Training / Serving modules There is a need to understand the amount of compute resources being used by a said workload inside a k8s cluster. Can a deep learning training task using some GPU / Memory / CPU, can allow another task (e.g. Serving) to be allocated on the same GPU / node pools. Shared allocation requires continuous monitoring/probing of resources that should result in useful metrics, which can be used to gauge infrastructure scaling events and cost estimations. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:9:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"},{"categories":["kubeflow"],"content":"Configurable tear down of Infrastructure after use An interesting theme, common to all teams as described before is cost control. Large or small organizations alike would like to only pay for infrastructure, till its useful. Which means if cloud instances / resources are not used anymore then its best to shut them down. Since cloud infrastructure is not directly tied to the lifecycle of the service, even after a training job (TFJob) finishes, the instances are left idle. This might incur unnecessary cost to an organization. Also native solution around scheduled downscaling of Serving Infrastructure during low RPS or using spot instances (targeted instance choices) for marginal redundancy can help significantly in cost reduction. These use-cases are common to a K8s app and we can write custom auto-scalers and/or bring about improvements to upstream auto-scalers to ease Kubeflow adoption significantly. We truly believe that Kubernetes is the tool to truly democratize big data and AI. Toolkits like Kubeflow really reinforces the dream where running AI tasks and serving them is not just limited to a handful of organizations but it is easily accessible to everyone. We would like to continue our efforts towards exploring Kubeflow, as we plan to ship it in our production cluster soon. ","date":"2018-11-09","objectID":"/why_kubeflow_in_your_infrastructure/:10:0","tags":["kubeflow","machine learning","kubernetes","infrastructure","deep learning"],"title":"Why Kubeflow In Your Infrastructure","uri":"/why_kubeflow_in_your_infrastructure/"}]